{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning (part I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that reinforcement learning relies on the interaction of the learning algorithm (which we call agent in this framework) and its environment by means of the actions the learner takes, which impact the environment and the rewards he received in return for his actions.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"reinforcementImage.png\",width=300,height=300>\n",
    "\n",
    "Image source: [Reinforcement Learning Coach](https://nervanasystems.github.io/coach/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I : Greedy approach and $\\varepsilon$-Greedy approaches\n",
    "\n",
    "#### Exercise I. Stationnary approach\n",
    "\n",
    "We will start by considering a simple k-armed bandit problem such as the one discussed in class. Here we take $k = 4$ and we take the reward to follow a Gaussian distribution with mean $mu_i$ and $\\sigma = 1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu1 = 0 # put your choice for the value \n",
    "# (the mean of the distribution). We assume the distribution of the \n",
    "mu2 = 0\n",
    "mu3 = 0\n",
    "mu4 = 0\n",
    "\n",
    "sigma1 = 1\n",
    "sigma2 = 1\n",
    "sigma3 = 1\n",
    "sigma4 = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "maxIter = 100\n",
    "iter = 0\n",
    "\n",
    "while iter < maxIter:\n",
    "    \n",
    "    \n",
    "    action = 0 # sample an action at random from 0 to 3\n",
    "    \n",
    "    reward = 0 # sample the reward according to the Gaussian distribution\n",
    "    \n",
    "    value  = 0 # update the value \n",
    "    \n",
    "    \n",
    "    iter +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise I.2. Non stationnary version\n",
    "\n",
    "Now code the non stationnary version of the k-bandit algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu1 = 0 # put your choice for the value \n",
    "# (the mean of the distribution). We assume the distribution of the \n",
    "mu2 = 0\n",
    "mu3 = 0\n",
    "mu4 = 0\n",
    "\n",
    "sigma1 = 1\n",
    "sigma2 = 1\n",
    "sigma3 = 1\n",
    "sigma4 = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "maxIter = 100\n",
    "iter = 0\n",
    "\n",
    "while iter < maxIter:\n",
    "    \n",
    "    \n",
    "    action = 0 # sample an action at random from 0 to 3\n",
    "    \n",
    "    reward = 0 # sample the reward according to the Gaussian distribution\n",
    "    \n",
    "    value  = 0 # update the value \n",
    "    \n",
    "    \n",
    "    iter +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise II. Escape room\n",
    "\n",
    "\n",
    "In this exercise, we will tackle a simple reinforcement learning problem. Consider the map given below. There are 5 rooms + the garden. We would like to train an agent to get out of the house as quickly as possible. To set up the evironment, we will consider 6 possible state (the rooms in which the agent is located) and 6 possible actions (moving from one room to any other room). \n",
    "\n",
    "The Q-table can thus be encoded by a $6$ by $6$ matrix. We will consider three types of rewards. Impossible moves (example 1 to 4) will be penalized by $1$. possible moves will be associated to a $0$ reward. Finally any move leading to an escape (e.g. 2 to 6) will be rewarded by 100. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Map\n",
    "<img src=\"QLearningImage2.png\" width=\"700\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question II.1\n",
    "\n",
    "As a first approach, we will just run a couple of pure exploration iterations. Just fill out the loop below and run a couple of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "done = False \n",
    "\n",
    "while not done: \n",
    "    \n",
    "    \n",
    "    '''complete the greedy steps by sampling an action at random and updating the state of the environement\n",
    "    until the variable Done is not set to True. Set this variable to True when the agent is able to escape the house'''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question II.2\n",
    "\n",
    "Now that you can solve the greedy approach. We will start to exploit and we will do that through the use of a $Q$ table. In this case, as indicated in the statement of the exercise, the Q-table is 6x6. Train the agent by alternating between exploitation and exploration. \n",
    "\n",
    "Since we want to update the $Q$-table, we will now add a line of the form \n",
    "\n",
    "$$Q[s, a] \\leftarrow (1-\\alpha)Q[s,a] + \\alpha\\left(R[a] + \\gamma\\max_{a'}Q[s',a']\\right)$$\n",
    "\n",
    "When in the exploration framework, we will sample the action at random as in Question III.1. When in the exploitation framework however, we will simply choose the action as the one that maximizes the entry in the $Q$-table for the particular state at which we are. Hence we have $a^* = \\underset{a}{\\operatorname{argmax}} Q[s,a]$. \n",
    "\n",
    "\n",
    "Code this epsilon-greedy approach below. You can start $\\epsilon =0.8$ \n",
    "Take a sufficiently small learning rate (you can for example start with 0.5) and a relatively large discount factor $\\gamma=0.9$ (You can later change those values to see how they affec the learning)\n",
    "\n",
    "Once you are done with the algorithm, try a couple of different values for $\\epsilon$ and describe the evolution in the learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "done = False \n",
    "\n",
    "epsilon = 0\n",
    "gamma = 0\n",
    "alpha = 0\n",
    "\n",
    "for episode in range(NumEpisodes):\n",
    "\n",
    "    done =False \n",
    "    \n",
    "    while not done: \n",
    "    \n",
    "    \n",
    "    '''Draw a number at random from the uniform distribution between 0 and 1''' \n",
    "    \n",
    "    \n",
    "    '''If the number is less then epsilon, explore if it is larger, exploit'''\n",
    "    \n",
    "    if randomDraw < epsilon:\n",
    "        \n",
    "        # exploration\n",
    "        \n",
    "        '''update the Q-table'''\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # exploitation\n",
    "        \n",
    "        '''update the Q-table'''\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III : Introduction to Q-Learning.\n",
    "\n",
    "\n",
    "There are several libraries in python including RLLib, pybrain,... that can be used to code reinforcement learning approaches. When Starting, a good approach is to consider the [Gym toolkit](https://gym.openai.com/) from openAI. Gym is compatible with both Theano and TensorFlow and contains a collection of examples that can be used to illustrate most of the reinforcement learning frameworks. Install gym with \"pip install gym\" (you may need !pip install cmake 'gym[atari]') or use \n",
    "\n",
    "\"git clone https://github.com/openai/gym\"\n",
    "\n",
    "\"cd gym\"\n",
    "\n",
    "\"pip install -e .\"\n",
    "\n",
    "if you prefer to clone the git repository. \n",
    "\n",
    "\n",
    "#### Exercise III.1. Gym Self Driving cab\n",
    "(based on the [learndatasci](https://www.learndatasci.com/) tutorials)\n",
    "\n",
    "As a starting point, we will consider the [self driving cab](https://gym.openai.com/envs/Taxi-v2/) example. Use the lines below to display the map for this particular example. The objective in this exercise is to train the cab through RL in order to (1) Drop off the passenger at the right location (2) save as much time as possible by taking the shortest path from the pick up to the drop off location and (3) respect traffic rules. \n",
    "\n",
    "- The cab is represented by the yellow rectangle. It is free to move on a 5x 5 grid and its spatial state can thus be described by a dimension 25 vector. \n",
    "\n",
    "- Wherever it is, the cab has four possible destinations, the four positions 'R', 'Y' 'G' and 'B'.  \n",
    "\n",
    "- We will further assume that the passengers can be picked up in any of the four locations R, G, Y and B. On top of those four locations, we also need to account for the framework in which the passenger is inside the cab. Any passenger position can thus be encoded by 5 binary variables.\n",
    "\n",
    "\n",
    "In this case, the state of the environment can thus be encoded by $5\\times 5 \\times 4 \\times 5$ binary variables. \n",
    "\n",
    "\n",
    "- Finally we need to encode the possible actions that the cab can take. At each location the cab can move in each of the four directions - east, west, north, south but it can also pick up or dropoff a passenger. We can thus encode the actions of the cab through 6 binary variables. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v2\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The cab is not supposed to cross the vertical bars which are representing wall and we will thus enforce this by setting the reward associated to impossible moves to -1__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gym lets us access the environement by means of the variable 'env'. The variable comes up with 3 methods. \n",
    "\n",
    "- env.reset\n",
    "- env.step (apply a step)\n",
    "- env.render (display the current state of the environment)\n",
    "\n",
    "You can also use env.action_space as well as env.observation_space to respectively access the set of actions and existing states of the environment. \n",
    "\n",
    "Use the first and third methods to reset and display the original state of your environment after resetting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of this first exercise is for the agent to learn a mapping from the existing states to the optimal actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step I. Interacting with and displaying the environment.__ \n",
    "\n",
    "Each state o fthe environment can either be encoded as a single number (between 0 and 499) or as a (5,5,5,4) tuple of the form (cab row, cab col, passenger index, direction). To move between the two, gym provides teh method 'encode' of the variable 'env'. Using the lines below, together with the render method discussed above, set and display a couple of environment states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state = env.encode(0,0,0,0) # change the 4 tuple to the state you wan to encode \n",
    "env.s = state\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step II. Taking actions based on rewards__ \n",
    "\n",
    "To each state of the environment is associated a Reward table which can be accessed through the line env.P[n] where n is the number encoding a particular state of the environment. Look at the reward tables of the states you rendered above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code (one line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward table has 5 rows (encoding the actions) and four columns of the form (probability, nextstate, reward, done). In this framework we don't consider any probability so this variable is always set to $1$. The last column indicates when the cab has droped a passenger at the right location. \n",
    "\n",
    "Each successful dropoff concludes one episode. \n",
    "\n",
    "\n",
    "#### Exercise III.1. \n",
    "\n",
    "Implement a full episode. That is we want an infinite loop that stops when the passenger has been droped. \n",
    "\n",
    "(hint: to sample an action you can use the method 'env.action_space.sample()'. Then note that env.step returns a four tuple of the form (state, reward, done, info) where 'done' indicates whether the passenger has been droped.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''This script should run one episode in which the cab takes random actions \n",
    "until the passenger is droped at the right location'''\n",
    "\n",
    "while not done: # change the condition for the loop to stop when the state \n",
    "    \n",
    "    \n",
    "    # put your code here\n",
    "    \n",
    "    \n",
    "    \n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have stored all the frames, use the lines below to play the resulting movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'].getvalue())\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise III.2. \n",
    "\n",
    "We will now see how one can exploit the previous experience of our agent to increase the rewards over time through Q-learning. In Q-learning the idea is to keep track of the actions that were beneficial by updating a mapping from any pair (environment state, action) to some number encoding the value of the pair. Q-values are updated following the equation \n",
    "\n",
    "\\begin{align}\n",
    "Q(\\text{state}, \\text{action}) &\\leftarrow (1-\\alpha) Q(\\text{state}, \\text{action}) \\\\\n",
    "&+ \\alpha (\\text{reward} + \\gamma \\max_a Q(\\text{next state}, \\text{all actions}))\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is we not only try to maximize the immediate reward but we also try to look for the action that will lead to the highest potential reward one step ahead. In the equation above, $\\alpha$ can be interpreted as a __learning rate__. $\\gamma$ which is known as the __discount factor__ indicates how much importance we want to give to the future rewards.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __Q-table__ is a table with 500 rows corresponding to the 500 states and 6 columns encoding each of the 6 actions. We will use a numpy array of zero to encode this table. Finally in order for our learning algorithm to be efficient, we will alternate between exploitation (with probability epsilon) and exploitation with probability (1-epsilon). \n",
    "\n",
    "\n",
    "Extend the \"random cab\" episode from Exercise II.1. in order to account for the Q table. \n",
    "\n",
    "- Use the line 'next_state, reward, done, info = env.step(action)'  to update the environment \n",
    "- Select the action either at random or according to the Q-table\n",
    "\n",
    "(Hint: to decide between exploration and exploitation, split the $[0,1]$ interval between a $[0,\\varepsilon]$ subinterval and a $[\\varepsilon,1]$ subinterval. Then draw a number uniformly at random from the $[0,1]$ interval. If the number falls in $[0,\\varepsilon]$ interval then pick an action at random. Otherwise,  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''This script should code one episode in which a random action is \n",
    "taken with probability epsilon and the action maximizing Q is taken with probability (1-epsilon)'''\n",
    "\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "\n",
    "        # complete the episode here\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise III.3. Evaluating the agent \n",
    "\n",
    "Once you have learned the Q-table, evaluate the agent behavior by choosing at each the step and in each state, the action that maximizes the value of the Q-table and play the resulting movie using the "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
