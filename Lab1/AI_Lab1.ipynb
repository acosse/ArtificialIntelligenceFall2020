{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Intelligence Week 1: Knowledge based agents\n",
    "\n",
    "\n",
    "In this exercise, we will implement a very simple environment. We consider a simple vacuum cleaner (VC) world. The VC world is summarized as follows.\n",
    "\n",
    "\n",
    " - Percepts: In this simple problem, our agent receives \n",
    " - Only three possible actions : move left, move right and suck up the dirt\n",
    " - Goals: The goal for each agent is to clean up the dirt. To be precise, the performance measure will be 100 points for each piece of dirt vacuumed up, minus 1 point for each action taken\n",
    " - Environment: The environment consists of a grid of squares or in this first exercise, only two squares (cellA and cellB). \n",
    " \n",
    "In the first exercise, we will consider each of the following agents:\n",
    "    \n",
    "   - A Random Agent (This one just takes actions at random without looking at the enbvironment)\n",
    "   - A Reflex Agent that relies on some optical sensor (moves right when on the left and left when on the right and suck up the dirt when there is some)\n",
    "   - A Table Lookup agent that keeps track of the possible sequences of percept and the associated actions it has to take for each of those sequences.\n",
    "    \n",
    "An illustration of the simplest vacuum cleaning environment is given below. In this figure, the cleaning device is in the same room as the dirt.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"vacuumCleaning.png\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.  The one dimensional case\n",
    "\n",
    "#### Step 1. random actions\n",
    "\n",
    "first code a function which samples an action at random. The function should take as input a list of possible actions and return an action at random. A smart way to do this is by returning a function of the variable percept which is a simple constant. You can do this with the function lambda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['forward']\n",
      "['Off']\n"
     ]
    }
   ],
   "source": [
    "actions = ['Right', 'Left', 'Suck']\n",
    "\n",
    "from random import sample\n",
    "\n",
    "\n",
    "def randomAction1(actions):\n",
    "    \n",
    "    '''complete'''\n",
    "    \n",
    "    \n",
    "    \n",
    "actionSelected = randomAction(actions)\n",
    "print(actionSelected)\n",
    "\n",
    "\n",
    "# an alternative is to return a function of the percepts that takes the percept object and return a random \n",
    "# action whatever the percept is \n",
    "\n",
    "def randomAction2(actions):\n",
    "    \n",
    "    '''complete'''\n",
    "\n",
    "actionSelection = randomAction2(actions)\n",
    "print(actionSelection([]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. The random agent.  \n",
    "\n",
    "Define the class Agent which initialize an Agent with a particular program (random action choice in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    \n",
    "    '''complete the initialization'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomAgent(actions):\n",
    "    \n",
    "    '''should return a random Agent of the class Agent'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. An optical sensor. \n",
    "\n",
    "We want to improve the program of our agent. We now want to make use of the optical sensor. We will define a new function Agent2 which should return an agent with an associated program which takes a percept and return the action. percept here contains two pieces of information: the location(cellA or cellB) and the status (Clean vs Dirty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Right'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def Agent2():\n",
    "    \n",
    "    '''should return an agent that makes use of its sensor. If in cellA, \n",
    "    should move right, in cellB, should move left. If dirt, should make use of optical sensor '''\n",
    "        \n",
    "    return Agent(program)\n",
    "\n",
    "\n",
    "\n",
    "agent2 = Agent2() \n",
    "agent2.program(['cellA', 'Clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4. Defining the environment. \n",
    "\n",
    "Each environment object will contain a basic 'step' method as well as a variable that encodes the state of the environment (which is the same as the percepts that the agent receives in this case). In this environment, we will consider a score of 10 each time the robot successfully cleans the dirt and a score of -1 each time the robot makes a move. \n",
    "\n",
    "Don't forget to add a initialization function to each new environment. For this exercise, the initialization should define the original 'status' of the environment, indicating whether there is dirt or not on each of the two cells. (you can set it as a dictionnary)\n",
    "\n",
    "Once we have this, we are ready to define the step function which updates the environment one step in time. Recall that in this simplified environment we only have 3 possible actions : Move right, move left and suck up the dirt. After each action we want to update the position of the agent and the state of the environment. Select the proper action based on the percepts.\n",
    "\n",
    "We assume that if the agent moves right it ends up at position B. If it moves left, it ends up in position A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['cellB'], 'Clean')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class environment:\n",
    "    \n",
    "    '''environement corresponding to the random agent or agent with optical sensor'''\n",
    "                    \n",
    "environment = environment(agent2)\n",
    "environment.step('Right')\n",
    "environment.perceptFun()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5. Table lookup agent\n",
    "\n",
    "We now would like to keep track of the sequence of past percepts and to define actions for each sequence of past percepts of length at most 2. The table should take the form of a dictionnary with entries corresponding to sequence of length up to 2 of the form percept = (position, dirt = YES/NO) and entries given by the action. E.g. (cellA, 'dirt') should lead to the action 'Suck'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LookUpTableAgent():\n",
    "    \n",
    "    percepts = []\n",
    "    \n",
    "    table = '''define the table'''\n",
    "    \n",
    "    def program(percept):\n",
    "        \n",
    "        percepts.append(percept)\n",
    "        \n",
    "        '''return the action corresponding to the percept entry in table'''\n",
    "        \n",
    "    return Agent(program)\n",
    "\n",
    "\n",
    "agentLookup = LookUpTableAgent()\n",
    "\n",
    "'''run the environment with agent1'''\n",
    "\n",
    "# Modify the environment so that the agent now keeps track of a sequence of percepts\n",
    "\n",
    "\n",
    "class LookUpEnvironment:\n",
    "    \n",
    "    \n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        \n",
    "        self.percepts = []\n",
    "        self.status = {'cellA' : random.sample(['Clean', 'Dirty'], 1)[0], \n",
    "                      'cellB' : random.sample(['Clean', 'Dirty'], 1)[0]}\n",
    "        self.percept = ('cellA', self.status['cellA'][0])\n",
    "        \n",
    "    \n",
    "    def step(self, agent):\n",
    "    \n",
    "        self.agent.percepts.append(self.percept)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2. $2\\times 2$  grid \n",
    "\n",
    "In this second exercise, you are asked to extend the 2 cell example from above to a $2x2$ grid. At most two squares will contain dirt and the agent should start in the upper left corner, facing to the right. Now we consider a 3D percept vector with entries given by the oucome of the photosensor, the oucome of the infrared sensor which is 1 if the robot is back home and 0 otherwise and the touch sensor which is 1 if the robot bumps into something (like a wall). \n",
    "\n",
    "In theory there are actions available: go forward, turn right by 90°, turn left by 90°, suck up dirt, and turn off. Yet will neglect the on/off. \n",
    "\n",
    "Run the environment simulator on the table-lookup agent in all possible worlds (how many are there?). Record its performance score for each world and its overall average score.\n",
    "\n",
    "The idea in this case is that the agent can always complete the task in less than 9 actions (4 moves, 3 turn and 2 suck ups) so your lookup table should not contain more than $2^9$ entries with an associated action for each of those percept entries and the corresponding score after this action has been taken. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Exploration_environment:\n",
    "\n",
    "    '''The 2x2 world'''\n",
    "    def __init__(self, agent):\n",
    "        \n",
    "        Table = np.zeros((2**9,2))\n",
    "        self.percept = np.zeros() # should be a sequence of length equal to the 9 previous percepts (dirt or not)\n",
    "        \n",
    "        \n",
    "        self.score = 0 # score should be -1 for each move and 10 each time some dirt has been sucked up\n",
    "    \n",
    "    \n",
    "    def get_Table(self):\n",
    "    \n",
    "        '''Return the table'''\n",
    "        \n",
    "    def update_Table(self):\n",
    "        \n",
    "        '''Store current score in the table'''\n",
    "        \n",
    "    \n",
    "    \n",
    "def TableDriven2by2Agent(table):\n",
    "    \n",
    "    \n",
    "    return Agent()\n",
    "\n",
    "\n",
    "class Exploitation_environment:\n",
    "    \n",
    "    '''The class for the exploitation of the Lookup Table''' \n",
    "    \n",
    "    def __init__(self,agent):\n",
    "        \n",
    "        '''To be completed'''\n",
    "        \n",
    "        \n",
    "    def step(self, action)    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# run the agent and store the results (score) in the table \n",
    "\n",
    "\n",
    "while EpisodeCompleted != 1:\n",
    "\n",
    "    environment.step() # the table should be updated at each step \n",
    "    \n",
    "\n",
    "table = environment.getTable()\n",
    "\n",
    "\n",
    "Exploitation_environment.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 3. $m \\times n$  grid\n",
    "\n",
    "Extend the result of the previous exercise to an mxn grid. You don't need to simulate the environment for m,n>2 in practice but your code should make such simulation possible in theory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same as above, nxn grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4. Adding furnitures\n",
    "\n",
    "We now want to add obstacles such furnitures. Each \"go forward\" action moves one square unless there is an obstacle in that square, in which case the agent stays where it is, but the touch sensor goes on. Add a penalty for when the agent bumps into an object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
