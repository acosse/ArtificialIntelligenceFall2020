{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Exercises: Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I. Natural Language Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"nlpImage.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "image credit:  [hackerearth.com](https://www.hackerearth.com/blog/developers/natural-language-processing-components-and-implementations/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 4a__\n",
    "\n",
    "In this exercise, you will get to use a support vector classifier to discriminate between positive and negative movie reviews. \n",
    "\n",
    "More specifically, you will get to first code your own term frequencyâ€“inverse document frequency (TF-IDF) statistic from the document.  \n",
    "\n",
    "\n",
    "The data for this exercise is stored in the file 'trainingSimpler.txt'. Start by loading this corpus. \n",
    "\n",
    "We will need to do some pre-processing. In particular we will use functions from the natural language toolkit (NLTK). \n",
    "\n",
    "\n",
    "1. We now need a way to encode the sentences in order to be able to compare them to one another. To do this, we will split each sentence into a list of words. This can be done using the class 'TweetTokenizer' from NLTK. An instance of that class is initialized below. Once you have read a line (document) from the corpus, you can split that document into words by using a call to the function tokenize from TweetTokenizer.  \n",
    "\n",
    "2. As a first step, you should just read each sentence from the file (corpus) and store the target/sentiment (1 vs 0) which appears at the end of each line in a separate numpy vector (called 'targets' below). Save the vector as a numpy array of integers. Keep all the other words and store them in a temporary variable\n",
    "\n",
    "3. As a second step, we will remove from each sentence the so-called \"stop words\" as well as the punctuation which do not carry any valuable information regarding the sentiments. To prune the sentence we will use two lists. The first one from the corpus module of nltk and the second one from the second one (encoding the punctuation) from the string module. To prune the lines (or documents), remove all the words that appear in the lists 'stopwords.words('english')' or 'string.punctuation'. \n",
    "\n",
    "4. Two dictionnaries are initialized below. We will use the first one, 'wordfreq' to keep track of the words and their number of occurence in the whole corpus. This first dictionnary should thus be of the form wordfreq[word] = num_occurence. In the loop, you should therefore add a line that for each word in the current list of words extracted from the current line in the file, either adds that word to the dictionnary with 1 as its occurence, or add 1 to the previous number of occurence for that word if the word already belongs to the dictionnary.\n",
    "\n",
    "5. We will also use the loop to build a second dictionnary, called 'corpus' which is simply a list of the pruned sentences. This dictionnary should be as follows corpus[k] = 'pruned list of words' corresponding to $k^{th}$ sentence in the file.   \n",
    "\n",
    "  \n",
    "\n",
    "N.B While you are reading the file, line by line, also keep track of the number of lines and store that number in a variable num_lines or num_sentences. We will use this variable later.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf=TfidfVectorizer()\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "f = open('trainingSimpler.txt')\n",
    "        \n",
    "wordfreq = {}\n",
    "corpus = {}\n",
    "ll = 0\n",
    "targets = []\n",
    "\n",
    "for line in f:\n",
    "    line = line.rstrip('\\n')\n",
    "    \n",
    "    \n",
    "    # put your code here\n",
    "    \n",
    "    \n",
    "        \n",
    "num_sentences = ll \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 4b__ As a second step, we will use the 'wordfreq' to retain as features, the 200 most frequent words from the total corpus. \n",
    "\n",
    "One approach is to use the functions .values( ) and .keys( ) to respectively access the values and the words from 'wordfreq'. Once you have the list of values, you can just use 'argsort' to return indices of the sorted values. [Be careful that arsort sorts in ascending order]\n",
    "\n",
    "Then build a new list of words by taking the words wordfreq.keys( )[Indice1] to wordfreq.keys( )[Indice200].\n",
    "\n",
    "You should finish this step with a list of the 200 most frequent words from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_freq_values = wordfreq.values()\n",
    "\n",
    "# put your code here\n",
    "\n",
    "most_freq = # complete here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Execise 4c.__ We will now define both the TF (term frequency) and IDF (inverse document frequency) values. \n",
    "\n",
    "- The Inverse document frequency (IDF) value is a measure of how much value any given word provides. The IDF index is computed as \n",
    "\n",
    "\\begin{align}\n",
    "\\log\\left(\\frac{\\text{Total number of docs}}{\\text{num. of docs in which word appears}}\\right)\n",
    "\\end{align}\n",
    "\n",
    "- The term frequency (TF) is usually simply given by the number of times a particular word appears in a document. \n",
    "\n",
    "The idea of TF-IDF is then to weight the term frequency by the Inverse document frequency. This gives a set of feature vectors that can be used to classify the documents. \n",
    "\n",
    "- The TF-IDF statistic is then computed as the product of the TF and IDF values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a). TF index__ To compute the TF index, we will again use a dictionnary, called 'word_tf_values' below. We will loop over all of the 200 most frequent words, then for each of the most frequent words, we will loop over all the (pruned) sentences from your variable 'corpus' and for each pruned sentence, we will loop over all its words and simply count the number of times the current 'most frequent word' appears in the sentence. For each sentence and each most frequent word, store the value in a variable 'doc_freq'. \n",
    "\n",
    "- Once you have read the whole line (or document), store the value 'doc_freq/len(corpus[line])' (that is the relative frequency of occurence of the current most_frequent word in the sentence) in a variable word_tf\n",
    "\n",
    "- Repeat this for each sentence and store the variables word_tf in a vector of length 'number of lines'. \n",
    "\n",
    "- After you have looped over all the sentences for the current 'most frequent word', store the vector encoding all the word_tf indices for each sentence in the dictionnary at the entry corresponding to the most frequent word. I.e. word_tf_values[most frequent word] =  tf_vector\n",
    "\n",
    "\n",
    "You should end up with a dictionnary of size '200 x number of documents/sentences'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "word_tf_values = {}\n",
    "\n",
    "kk = 0\n",
    "for token in most_freq:\n",
    "    \n",
    "    sent_tf_vector = []\n",
    "    ll = 0\n",
    "    for line in range(0,num_sentences):\n",
    "        doc_freq = 0\n",
    "        for word in corpus[line]:\n",
    "            \n",
    "            # put your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b) IDF value__ For the IDF values, the principle is even simpler. We only need to compute the ratio between the total number of documents in the corpus and the number of documents in which the current 'most frequent' word appears. \n",
    "\n",
    "Again, we will store the result in a dictionnary 'word_idf_values' whose keys are the most frequent words and whose values will be the IDF index. \n",
    "\n",
    "\n",
    "Run over all the most frequent words. For each of the 200 most frequent words, run over all the lines from the corpus. Then if the most frequent word appears in the line, increment a counting variable (for example 'doc_containing_word') by 1 (you can just use the condition 'if token in corpus[line]:' for line that runs from 0 to the total number of lines in the file)\n",
    "\n",
    "Once you have run over all the lines for the current most frequent word, add the entry in the dictionnary as\n",
    "\n",
    "word_idf_values[token] = float(np.log(len(corpus[line]))/float((1 + doc_containing_word)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_idf_values = {}\n",
    "for token in most_freq:\n",
    "    \n",
    "    \n",
    "    # put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c)TF-IDF__ Finally we compute the TF-IDF feature vectors for each sentence. The TF-IDF representation of each sentence can be viewed as a feature vector whose entries give us an indication of the weight of any of the most frequent word in that sentence as well as the importance of each word in terms of the meaning of the sentence. \n",
    "\n",
    "To build the TF-IDF representation of each sentence, we will use two variables :\n",
    "\n",
    "'tfidf_values' and 'tfidf_sentences' \n",
    "\n",
    "Loop over the list of most frequent words. For each most frequent word, loop over the list of documents (or sentences from the txt file). Store the product of the tf index by the idf index for the current word and sentence in a variable (e.g. 'tf_idf_score'). Append those values to teh variable 'tfidf_sentences' through the loop on sentences, thus obtaining a vector of size 'num_sentences'. \n",
    "\n",
    "Then append those vectors to the variable 'tfidf_values', through the loop on the most frequent words, thus obtaining a '200' by 'num_setences' array.  \n",
    "\n",
    "\n",
    "You should end up with a 2D numpy array of size 'num_sentences' by 'num_most_frequent_words' that encodes each sentences as a 1 by 200 feature vector. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_values = []\n",
    "for token in most_freq:\n",
    "    tfidf_sentences = []\n",
    "    for tf_sentence in range(0,num_sentences):\n",
    "        tf_idf_score = word_tf_values[token][tf_sentence] * word_idf_values[token]\n",
    "        tfidf_sentences.append(tf_idf_score)\n",
    "    tfidf_values.append(tfidf_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__d) Support vector classifier__ Now that we have our feature representation, we can train our classifier. Initialize a SVC classifier from scikit learn. Start by taking a RBF kernel with C=1000, gamma= 1. Then train it on your data. Make sure to transpose your TF-IDF array for it to be 'num_samples' x 'num_features'. Then test if on your validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bonus I__ : Improving your classifier. The 'trainingSimpler.txt' corpus on which you applied you NLP classifier so far can be considered as relatively simple as the sentences are relatively different and always contain clear positive or negative statements. It is however not necessarily the case. An example of a more advanced dataset is given by the pair ('trainingMICH.txt';'testdataShortMICH.txt') which encode a variety of Sentiment Classification sentences from the [University of Michigan](https://www.kaggle.com/c/si650winter11). \n",
    "\n",
    "Try to apply your TF-IDF statistic on this more difficult dataset (note that we use a shorter version 'testdataShortMICH.txt' than the one that is provided on Kaggle). How does it work?\n",
    "\n",
    "What could you do to improve the efficiency of your classifier ? (hint: think of the redundant words that do not really bring any sentimental value to the sentences)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bonus II__ : Train a logistic regression classifier on your TF-IDF vectors and apply it to the validation set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part II. Convolutional neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.1 MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import MinMaxScaler, scale, StandardScaler, Normalizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "from keras.layers import Dense # Dense layers are \"fully connected\" layers\n",
    "from keras.models import Sequential # Documentation: https://keras.io/models/sequential/\n",
    "\n",
    "image_size = 784 # 28*28\n",
    "num_classes = 10 # ten unique digits\n",
    "\n",
    "\n",
    "\n",
    "# put your code here \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### II.2 Cats and Dogs \n",
    "\n",
    "Now that you are a little more comfortable with Keras, you will get to code and apply a neural network to some real dataset.\n",
    "\n",
    "You get all the points for this question if you can build the network, train it and apply it to the test set with reasonable results. \n",
    "You get 2 bonus points if you can score better than 85% on the test (or any given randomly slected subset)\n",
    "You get 1 more bonus point if you get the best scores when I test your model with any subset of images from the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you got familiarized with Keras, we can focus on a funnier instance of classification. In this exercise, we will learn to discriminate between pictures of cats and dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to kaggle and download the Microsoft dogs vs cats dataset: https://www.kaggle.com/c/dogs-vs-cats/rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.1.__ First write a function that runs through all the images and find the minimum size. You can start with the snippet below. To apply a neural network to our dataset, we will first need to  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here. You can make use of the lines below to load the images\n",
    "\n",
    "\n",
    "path = '/Users/acosse/Desktop/Teaching/MachineLearning2019/Assignment2/dogs-vs-cats/train'\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "for p in os.listdir(path):\n",
    "    category = p.split(\".\")[0]\n",
    "    img_array = cv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE)\n",
    "    new_img_array = cv2.resize(img_array, dsize=(80, 80))\n",
    "    plt.imshow(new_img_array,cmap=\"gray\")\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.2__ Now write a function that runs throught the images and (Hint: use the functions resize from cv2 and append from numpy) build a matrix storing the images after setting them to the appropriate minimum size (hint check the function 'cv2.resize')\n",
    "\n",
    "Dont forget to normalize the data (X = X/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.3.__ So far we have considered traditional Multilayer perceptrons. However, when dealing with images, it is usually much more interesting to use convolutional neural networks (see https://en.wikipedia.org/wiki/Convolutional_neural_network or http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf, Fig.2). convolution neural networks work by sliding a filter on the input image and computing local \"averages\". In the example below the input image is binary and the filter is made up of all 1. But in practical convolution neural networks, the network learns the green filter and the blue matrix represents our images\n",
    "\n",
    "The red image then represents the output to the filter (the sum of the product of the elements of the green filter by the elements of the image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![convolution1.gif](convolution1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for MNIST, there are three steps that we need to follow \n",
    "- First we need to build the network\n",
    "- We then need to define the solver used \n",
    "- Finally we train the network\n",
    "\n",
    "We can build a simple convolutional Neural network as follows:\n",
    "    - Take the Sequential model (this is the conventional model)\n",
    "    \n",
    "The architecture of a convolutional neural network usually involves 4 parts:\n",
    "\n",
    "   - Convolution\n",
    "   - Pooling\n",
    "   - Flattening\n",
    "   - Fully connected layer\n",
    "\n",
    "For the convolution part we will repeat __twice__ the following construction:\n",
    "    \n",
    "   - Add a first 2D convolutional layer (see https://keras.io/layers/convolutional/) with 64 3 by 3 filters and Relu activation function\n",
    "    \n",
    "   - To make the detection robust and to speed up the computation, a popular approach is to use MaxPooling (MaxPooling turns the output of the convolution layer into a reduced image by only retaining the max across a particular region). One intuition for maxPooling (given by Andrew Ng) is that if a feature (the ear of a cat/dog for example) is detected, is should be keps in the output (hence the max). Add a 2D maxPooling layer, after the convolutional layer, with 2 by 2 pooling windows\n",
    "    \n",
    "    \n",
    "Then, following the conventional CNN structure we Flatten the result (see model.add(Flatten())). \n",
    "\n",
    "Finally, we add a dense layer of 64 neurons with Relu activation function. \n",
    "\n",
    "To define the final output, you can add a simple dense layer with only one neuron. As we only have two classes defined by a binary variable, we will choose our activation function to be the sigmoid. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Maxpool.png](Maxpool.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put the construction of the Keras network here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have built the network, we need to define our training algorithm. This is done with a call to model.compile (see https://keras.io/getting-started/sequential-model-guide/#compilation)\n",
    "\n",
    "You can modify the parameters but to start, a good choice is to choose 'adam' as your solver, and the binary cross entropy as your loss. Also set metrics=['accuracy'] to keep track of the training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to train your model using a call to the 'fit' function of this model. Again up to you if you want to modify the parameters but a good start can be to take 10 epochs and to take a batch of size about 30. Also check the 'validation_split' argument (https://keras.io/models/sequential/). This might give you an indication of how well your model is doing through training. You might also want to only consider a subset of the image when training as the data set is quite large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
