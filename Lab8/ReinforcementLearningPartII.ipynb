{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning: Summary + additional exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to extend our reinforcement learning algorithm for the autonomous cab, by learning the Q-table as a neural network. This idea which is known as deep Q-learning was first introduced by DeepMind (see for example [paper1](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)). \n",
    "\n",
    "\n",
    "#### Non parametric model and Time Difference (TD) learning\n",
    "\n",
    "Recall that the Bellman equation for fixed policy reads as \n",
    "\n",
    "\\begin{align}\n",
    "U^\\pi[s] = R[s] + \\gamma \\sum_{s'} P(s'|s, \\pi(s)) U^\\pi[s']\n",
    "\\end{align}\n",
    "\n",
    "We can use this equation (as in the bandit algorithm) even if we do not have access to a model for the transition probabilities to update our estimate for the value function $U[s]$. Indeed, if every time we are at $s$ we always move to $s'$, we would expect the relation between $U[s]$ and $U[s']$ to satisfy $U[s] = R[s] +\\gamma U[s']$. \n",
    "\n",
    "In order to learn a value function that satisfies the Bellman equation, we can thus update our estimate each time a transition occurs from a state $s$ to a state $s'$ as \n",
    "\n",
    "\\begin{align}\n",
    "U^\\pi[s] \\leftarrow U^\\pi[s] + \\alpha(R[s] + \\gamma U^\\pi[s']-U^\\pi[s])\n",
    "\\end{align}\n",
    "\n",
    "This approch is sometimes known as temporal difference learning because it relies on the difference between successive estimates in the value function. \n",
    "\n",
    "In the equations above, we have assumed that we have access to the policy $\\pi(s)$. In practice, the agent often does not know what the optimal action is. In such a framework, not only does it have to learn the value of a state, but it also has to learn what action it should take in any given state. For that purpose, one approach is to introduce a $Q$ function. As before, we can introduce a Bellman equation that should be satisfied at equilibrium:\n",
    "\n",
    "\\begin{align}\n",
    "Q[s, a] = R[s] + \\gamma \\sum_{s'}P(s'|s, a)\\max_{a'}Q[s',a']\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "which we can try to satisfy by applying at each step, the Time difference rule\n",
    "\n",
    "\\begin{align}\n",
    "Q[s, a] \\leftarrow Q[s, a] + \\alpha (R[s] + \\gamma \\max_{a'}Q[s',a'] - Q[s, a])\n",
    "\\end{align}\n",
    "\n",
    "Here $R[s]$ should be understood as the reward associated to the action $a$ (i.e. R[s] = R[a]). \n",
    "\n",
    "An alternative to Q learning, SARSA (for $(s,a,r, s',a')$) waits until the action $a'$ has actually been taken and then update the Q-table as \n",
    "\n",
    "\\begin{align}\n",
    "Q[s, a] \\leftarrow Q[s,a] + \\alpha(R[s] + \\gamma Q[s',a'] - Q[s, a])\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "#### Parametric Learning\n",
    "\n",
    "As we saw earlier, it is not always possible to learn the $Q$-table. In many situations, it might be more convenient to replace the unknown table with a parametric model which which might enable faster learning. An example of such model can read as \n",
    "\n",
    "\\begin{align}\n",
    "\\hat{U}_\\theta[s] = \\theta_1 f_1[s] + \\theta_2f_2[s] + \\ldots + \\theta_n f_n[s]\n",
    "\\end{align}\n",
    "\n",
    "One can then replace the Time Difference update rule by updating our parametric model, each time the agent performs one step. That is to say, we want to reduce the difference between the two sides of the Bellman equation \n",
    "\n",
    "\\begin{align}\n",
    "Q[s, a] = R[s] + \\gamma \\sum_{s'}P(s'|s, a)\\max_{a'}Q[s',a']\n",
    "\\end{align}\n",
    "\n",
    "In order to update our parameters to reduce this difference, we apply at each step the rule\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_i \\leftarrow \\theta_i + \\alpha \\left[R[s] + \\gamma \\hat{U}_\\theta[s'] - \\hat{U}_\\theta[s]\\right]\\frac{\\partial \\hat{U}_\\theta[s]}{\\partial \\theta_i}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Corresponding to the minimization of the error \n",
    "\n",
    "\\begin{align}\n",
    "\\min_{\\theta} \\left[R[s] + \\gamma \\hat{U}_{\\theta^{\\text{old}}}[s'] - \\hat{U}_\\theta[s]\\right]\n",
    "\\end{align}\n",
    "\n",
    "A similar idea can be applied to the action value estimates\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_i \\leftarrow \\theta_i + \\alpha \\left[R[s] + \\gamma \\max_{a'}\\hat{Q}[s',a'] - \\hat{Q}_\\theta[a,s]\\right]\\frac{\\partial \\hat{Q}_\\theta[a,s]}{\\partial \\theta_i}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 1. Escaping again\n",
    "\n",
    "Consider a $10\\times 10$ environment with a $+1$ end state at $(10,10)$ and a $-1$ end state located at $(10,1)$. place a couple of obstacles at random in this environment. Consider starting from a fixed position. The actions are deterministic moves in the four possible directions. We keep the reward -.04 for non terminal states and collision into a wall results in no movement. We want to learn an action-utility function. Consider a parametric model of the form \n",
    "\n",
    "\\begin{align}\n",
    "Q[s, a] = \\theta_1 x + \\theta_2 y +\\theta_3 a\n",
    "\\end{align}\n",
    "\n",
    "Study the learning of your agent. Then consider more advanced parametric models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 2. Memory\n",
    "\n",
    "It is usually more effcient to keep a memory of a few iterates $s_i,a_i,r_i$ and then do a few stochastic gradient steps on the memory rather than one step at a time. Try to modify your code from above to include such a memory (see [paper1](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) if you need more details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3. Back to the cab\n",
    "\n",
    "Apply the [deep Q-learning approach](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) to the cab exercise from the previous lab. Play with the parameters and display the evolution of the average reward and ratio of true completion time over actual completion time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
